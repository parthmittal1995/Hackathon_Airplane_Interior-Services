# -*- coding: utf-8 -*-
"""topic-modeling-using-gensim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8tEGEocDK8hLhv1LiibVX7ODBjD0V6z
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
print(os.listdir("/content"))
import nltk
nltk.download('wordnet')
import csv
from tqdm import tqdm
import re
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import pickle
import nltk
from gensim import corpora, models
np.random.seed(42)

from google.colab import drive
drive.mount('/content/drive')

### Paths for the stop words and the data ###
stop_words_path = "/content/stop_words.pkl"
data_path = "/content/drive/MyDrive/master (1).csv"

def load_pickle(path):
  """ 
  Loads a pickled object 
  ***
  Arugements:
  1. path : path for the pickled file
  2. pickled_object : python object that was pickled

  """  
  pickle_file = open(path, "rb")
  while True:
      try:
          pickled_object = pickle.load(pickle_file)
      except EOFError:
          break

  pickle_file.close()
  return (pickled_object)

def preprocessing(sentence):
    """ 
    Preprocesses each setence. Lemmatizes each sentence  
  
    """  

    #Lemmatizes the word - Groups words with similar meanings together into one object
    # return stemmer.stem(WordNetLemmatizer().lemmatize(sentence, pos='v'))
    return WordNetLemmatizer().lemmatize(sentence, pos='v')

def preprocess(sentence):
      """ 
      Removes the stop_words from the sentences  
      """  

      result = []
      
      for token in gensim.utils.simple_preprocess(sentence):
          if token not in stop_words and len(token) > 2:
              result.append(preprocessing(token))
              
      return result

  


class topic_model:
  
  def __init__(self, prep_docs, stop_words_path):
    self.data = list(prep_docs)
    
  
  def create_corpus(self):
    """ 
    Creates a dictionary, bag of words, tf-idf list and a corpus containing the tfidf
      
    """  


    self.dictionary = gensim.corpora.Dictionary(self.data)
    self.dictionary.filter_extremes( no_below= 15 ,no_above=0.5, keep_n=100000)
    self.bag_of_words = [self.dictionary.doc2bow(document=document) for document in self.data]
    self.tfidf = models.TfidfModel(self.bag_of_words)
    self.corpus_tfidf = self.tfidf[self.bag_of_words]

    
  def train(self,num_topics = 7, passes = 5, chunksize = 100 ):
    """
    Trains the model
    ***
    Arguments:
    1. num_topics : Number of topics we want
    2. passes : Number of passes over the entire data while training
    3. chunksize : Batch size for each iteration
    """
    self.tfidf_model = gensim.models.LdaMulticore(self.bag_of_words, num_topics=7, id2word=self.dictionary, passes=5, workers=7, random_state= 42, chunksize = 100)

### Load the stop words ###

stop_words = load_pickle(stop_words_path)
data = pd.read_csv(data_path, error_bad_lines = False)

### Preprocess the data ###

preprocessed_documents = data['review'].astype(str).map(preprocess)

### Initialize class object and create the corpus ###

tp = topic_model(preprocessed_data, stop_words_path)
tp.create_corpus()

### Train the model ###

tp.train()

### Print the topics obtained ###

tp.tfidf_model.show_topics()

### Testing on unseen data
test = 'There was no space'

doc_bow = tp.dictionary.doc2bow(preprocess(test))
topics = sorted(tp.tfidf_model[doc_bow],key=lambda x:x[1],reverse=True)
#data_topics['text'] = text
# topics = [str(i[1]) for i in topics]
print(topics)

"""###Testing on Unseen Data"""

test_data = pd.read_excel('/content/raw_messages_for_test.xlsx')

def topic_scores(text):
    doc_bow = tp.dictionary.doc2bow(preprocess(text))
    # print (text)
    topics = sorted(tp.tfidf_model[doc_bow],key=lambda x:x[0],reverse=False)
    #data_topics['text'] = text
    # topics = [i[1] for i in topics]
    return topics

    # print(topics)
    # print("Score: {}\n  Topic: {}".format(tfidf_model.show_topics()[topics[0][0]], topics))

test_data['scores'] = test_data['full_message'].astype(str).apply(lambda x : topic_scores(x))
#topic_scores('asd')

test_data

# Function to append scores to empty lists #
list1 = []
list2 = []
def topic_scores(row):
  
  row = sorted(row, key=lambda x: (x[1]), reverse=True)
  list1.append(row[0][0])
  if (row[0][1] < 0.5):
    list2.append(row[1][0])
  else:
    list2.append(-1)

# Splitting the scores into the dominant topic and the secondary topic
for i, row in tqdm(enumerate(test_data['scores']), total = 19, position = 0, leave = True ):
  topic_scores(row)

# Adding the dominant topic and the secondary topic to the test data
topics = ['Inflight Meal', 'Cancellations & Booking', 'Entertainment & Premium Services', 'Boarding & Delays', 'Baggage & Cost', 'Interior & Seating', 'General Experience' ]
test_data['Dominant_Topic'] = list1
test_data['Second Topic'] = list2
test_data['topic'] = test_data.apply(lambda x: topics[int(x['Dominant_Topic'])], axis = 1)
test_data.to_csv('test_topic_model.csv')